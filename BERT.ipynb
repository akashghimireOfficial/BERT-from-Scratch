{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38fb8b9",
   "metadata": {},
   "source": [
    "## **BERT** \n",
    ">__[*original Turtorial Link*](https://www.tensorflow.org/text/tutorials/transformer)__\n",
    "\n",
    "### Turtorial Summary\n",
    "1. Downloading the Dataset\n",
    "2. Text tokenization & detokenization\n",
    "3. Setup of Input Pipeline \n",
    "4. Positional Encoding\n",
    "5. Buidling Transformer Model from Scracth\n",
    "6. Setting Hyperparameters \n",
    "7. Training and CheckPointing\n",
    "8. Attention Plot\n",
    "9. Exporting the saved model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ffeb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32d7cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b0ba5",
   "metadata": {},
   "source": [
    "#### Downloading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9555412",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading the Dataset\n",
    "## useing Tensorflow datasets\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "017a9efe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "95\n",
      "mas e se estes fatores fossem ativos ?\n",
      "38\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "48\n",
      "\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "111\n",
      "but what if it were active ?\n",
      "28\n",
      "but they did n't test for curiosity .\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 22:47:18.139060: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "##if got time look more for what does the take() method refers to \n",
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "    for pt in pt_examples.numpy():\n",
    "\n",
    "        print(pt.decode('utf-8'))\n",
    "        print(len(pt.decode('utf-8')))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))\n",
    "        print(len(en.decode('utf-8')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dce6c3",
   "metadata": {},
   "source": [
    "### tokenization & detokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e94c266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to train a NLP model first we need to tokenized them \n",
    "##Converting the text to sequence of token IDS \n",
    "## In simple words converting a sentence or words to list of unique tonek-IDS\n",
    "## for tekenization, in this turtorial we will be using frozen model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dcc11ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ted_hrlr_translate_pt_en_converter.zip'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.keras.utils.get_file(\n",
    "    f'{model_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c10c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading the model\n",
    "tokenizers=tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22b40e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n't test for curiosity .\"], dtype=object)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56254ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Example of tokenization \n",
    "encoded_en_examples=tokenizers.en.tokenize(en_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a330803",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "## encoded tokens of above en_examples\n",
    "for row in encoded_en_examples.to_list():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43146852",
   "metadata": {},
   "source": [
    "*the shape of each input array is not same as the lenght of the sentence is not fixed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f23c00f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "### Decoding the encoded english examples \n",
    "round_trip = tokenizers.en.detokenize(encoded_en_examples)\n",
    "for line in round_trip.numpy():\n",
    "    print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94cba068",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How does the sequence of words are tokenized \n",
    "## using lookup() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa9492c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n",
      "  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n",
      "  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n",
      "  b'##ity', b'.', b'[END]']                                                 ,\n",
      " [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n",
      "  b'[END]']                                                           ,\n",
      " [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n",
      "  b'curiosity', b'.', b'[END]']                                          ]>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizers.en.lookup(encoded_en_examples)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "076a03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................."
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for pt_examples, en_examples in train_examples.batch(1024):\n",
    "    pt_tokens = tokenizers.en.tokenize(pt_examples)\n",
    "    lengths.append(pt_tokens.row_lengths())\n",
    "    #print(pt_tokens.row_lengths())\n",
    "    en_tokens = tokenizers.en.tokenize(en_examples)\n",
    "    lengths.append(en_tokens.row_lengths())\n",
    "    print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9144f127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1024,), dtype=int64, numpy=array([45, 21, 23, ..., 21, 47, 35])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27,  9, 12, ...,  9, 23, 28])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 32,  22,  34, ...,  39, 101,  36])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17, 12, 20, ..., 30, 56, 13])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([48, 37, 27, ..., 92, 21, 49])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([29, 18, 15, ..., 53, 13, 29])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([72, 38, 35, ..., 20, 54, 13])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([38, 18, 16, ..., 11, 26, 11])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15,  24,  51, ...,  27,  40, 117])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 16, 24, ..., 20, 24, 71])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([28, 44, 26, ..., 95, 34, 32])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17, 28, 14, ..., 39, 23, 22])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 28, 43, ..., 71, 20, 18])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 15, 26, ..., 34,  7, 10])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 32,  60,  25, ...,  37, 101,  31])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([18, 29, 15, ..., 22, 42, 21])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 64, 26, ..., 38, 65, 41])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 28, 12, ..., 15, 25, 24])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([21, 74, 30, ..., 32, 84, 71])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([12, 40, 10, ..., 20, 30, 34])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([56, 54, 38, ..., 34, 22, 23])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([45, 22, 21, ..., 14, 10, 16])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 30,  25,  56, ..., 114,  34,  75])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 14, 30, ..., 47, 17, 34])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([26, 24, 28, ..., 17, 29, 30])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([14, 10, 13, ..., 10, 13, 13])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 25, 65, ..., 39, 56, 75])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 14, 33, ..., 15, 26, 39])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([48, 25, 23, ..., 65, 15,  8])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24,  9, 10, ..., 37, 10,  5])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([50, 35, 56, ..., 33, 23, 58])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([30, 19, 20, ..., 15, 11, 23])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 52, 11, ..., 43, 25, 37])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 34, 10, ..., 25, 11, 21])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([52, 83, 31, ..., 33, 98, 31])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([30, 38, 19, ..., 15, 34, 16])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 30,  14,  24, ..., 103,  17,  55])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17,  8, 34, ..., 51,  9, 37])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([36,  9,  8, ..., 55, 31, 37])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 10,  5, ..., 28, 13, 17])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 39,  34,  19, ...,  43, 115,  26])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 19, 10, ..., 29, 65,  9])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([51, 28, 57, ..., 48, 25, 41])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 16, 27, ..., 24, 12, 18])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([52, 32, 80, ..., 14, 62, 44])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 19, 42, ..., 11, 31, 20])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([34, 17, 49, ..., 12, 21, 25])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 12, 30, ...,  8, 11, 11])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 68, 119,  19, ...,  11,  21,  35])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([31, 54, 11, ..., 12, 12, 25])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([38, 25, 14, ..., 54, 21, 40])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([25, 10,  6, ..., 24, 10, 27])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 57,  41,  14, ...,  26,  92, 198])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 36,  20,  12, ...,  12,  40, 103])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([25, 30, 40, ..., 35, 36, 30])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 22, 21, ..., 22, 17, 13])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 26, 23, ..., 98, 32, 42])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 16, 12, ..., 52, 14, 23])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 65, 23, ..., 63, 47, 21])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 9, 34, 15, ..., 40, 24, 15])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([12, 21, 37, ..., 28, 16, 25])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10,  8, 16, ..., 11,  9, 12])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([99, 82, 41, ..., 33, 59, 15])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([41, 46, 22, ..., 13, 25,  8])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15,  25,  64, ..., 158,  25, 103])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 14, 44, ..., 78, 13, 40])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([28, 13, 78, ..., 25, 39, 12])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19,  9, 31, ..., 12, 22, 10])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([20,  9, 90, ..., 21, 66, 17])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11,  5, 34, ..., 12, 27, 10])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([127,  28,  35, ...,  75,  43,  14])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([87, 16, 12, ..., 34, 18, 13])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([  8,  27,  90, ...,  48,  10, 227])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([  5,  19,  38, ...,  23,   4, 130])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([65, 69, 37, ..., 13,  9, 11])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([32, 36, 19, ..., 13,  5, 10])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 37, 21, ..., 49, 15, 23])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([20, 17, 11, ..., 28, 13, 15])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 24, 75, ..., 40,  9, 15])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 8, 12, 53, ..., 21,  8, 10])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([21, 55, 24, ..., 36, 34, 36])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 27, 10, ..., 24, 19, 23])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([46, 38, 28, ..., 56, 53, 40])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 27, 11, ..., 26, 24, 21])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 42, 241,  50, ...,  64,  40,  54])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15, 117,  23, ...,  31,  24,  21])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 29, 29, ..., 49, 24, 63])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24,  8, 19, ..., 33, 13, 32])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([43, 27, 28, ..., 76, 51, 46])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([33, 13, 11, ..., 43, 24, 16])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 28, 21, ..., 21, 46, 45])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24, 16, 12, ..., 10, 23, 26])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([76, 26, 52, ..., 29, 53, 15])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([40, 19, 23, ..., 19, 20, 10])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([106,  15,  14, ...,  73,  25,  56])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([44,  9,  9, ..., 32, 18, 32])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([44, 30, 33, ..., 52, 42, 38])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([18, 13, 21, ..., 27, 16, 21])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 31, 12, ..., 98, 19, 24])>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 7, 19, 11, ..., 73,  9, 14])>,\n",
       " <tf.Tensor: shape=(585,), dtype=int64, numpy=\n",
       " array([ 25,  22,  11,  26,  34,  34,   8,  63,  15,  37,  71,  67,  18,\n",
       "         11,  53, 109,  84, 143,  20,  17,  30,  30,  19,  10,  46,  30,\n",
       "         24,  23,  22,  60, 153,  19,  69,  20,  18,  20,  40, 113,  54,\n",
       "          9,  20,  15,  43,  48,  55,  34,  55,  38,  56,  14,  24,  20,\n",
       "         45,  28,  32,  26,  18,  15,  40,  57,  32,  28,  39,  13,  30,\n",
       "         89,  18,  48,  57,  40, 128,  25,  36,  70,  94,  71,  85,  29,\n",
       "        148,  48,  39,  50,  33, 111,  19,  31,  19,  45,  34,  29, 100,\n",
       "         30,  28,  25,  66,  76,  36,  67,  18,  55,  50,  35,  85,  17,\n",
       "         51, 130,  19,  14,  33,  23,  84, 108,  34,  24,   9,  23,  18,\n",
       "         17,  32,  67,  21, 135,  24,  53,  57,  78,  43,  30,  36,  22,\n",
       "         54,  24,  34,  62,  65,  63,  10,  87, 151,  16,  53,  41,   9,\n",
       "         45, 138,  29,  67, 122,  31,  33,  27,  53,  17, 138,  54,  81,\n",
       "         33,  35,  81,  32,  46,  29,  25,  13,  55,  92,   9,  29,  48,\n",
       "         54,  25,  75,  91, 109,  56,  63,  54,   9,  65,  15,  39,  39,\n",
       "         23,  23,  78,  21,  60,  66,  65,  30,  22,  27,  51,  29,  31,\n",
       "         27,  42,  49,  31,  19,  13,  71,  25,  23,  81,  12,  88,  16,\n",
       "         99,  18,  31,  20, 106,  21,  11,  29,  71,  21,  47,  20,  19,\n",
       "         72,  14,  34,  58,  18,   9,  15, 248,  67,  50,  94,  22, 150,\n",
       "         49,  14,  37,  80,  47,  89,  29,  60,  30,  98,  24,  91,   9,\n",
       "         44,  26, 131,  43,  50,  10,  40,  19,  87,  58,  20,  17,  27,\n",
       "          8,  35,  15,  67,  33,  28,  11,  39,  27,  18,  39,  45,  37,\n",
       "         86,  27, 106,  36,  26,  20,  53,  31,  41,  24,  24,  21,  51,\n",
       "         18,  55,  10,  29, 178,  65,  33,  24,  99,  44,  91,  18,  24,\n",
       "         41,  28,  18,  37,  83,  23,  73,  21,  27,  35,  87,   7,  33,\n",
       "         79,  28,  29,  42,  75,  31,  28,  26,  17, 211,  19,  31,  94,\n",
       "         36,  26,  43,  24,  50,  40,  20,  42,  39, 169,   8,  53,  18,\n",
       "         37,  84,  26,  90,  78,  18,  53,  25,  28,  40,  16,  36,  24,\n",
       "         44,  32,  22,  59, 140,  57,  62,  29,  49,  48,  45,  20,  45,\n",
       "         47,  47,  57, 136,  29,  18,  62,  70,  14,  45,  38,  49,  74,\n",
       "         37,  23,  16,  34,  38,  19,  18,  16,  88,  32,  78,  49,  88,\n",
       "         26,  44,  16,  31,  30,  52, 137, 114,  33,  26,  22,  24, 172,\n",
       "         32,  28,  90,  15,  27,  91, 108,  52,   9,  11,  45,  24,  37,\n",
       "         99,  19,  24, 107,  48,  70,  22,  85, 111, 178,  30,  38, 118,\n",
       "         85,  10,  33,   8,  22,  29,  19,  28,  21, 177,  24,  55,  30,\n",
       "         17, 135,  27,  22,  31,  34,  20,  33,  34, 142,  23,  75,  45,\n",
       "         32,  21,  21,  15,  31,  32,  30,  10, 158, 148,   9,  45,  19,\n",
       "         55,  24,  47,  66,  16,  86,  55,  48,  53,  19,  21,  39,  22,\n",
       "         15,  20,  84,  29,  38,  77,  28,  79, 187,  57,  23,  61,  23,\n",
       "         74,  41,  55,  32,  79,  32,  30,  54,   7,  50,  16,  21,  25,\n",
       "         51,  31,  14,  16,  51,  35,  36,  16,  50,  56,  59,  47,  34,\n",
       "        140,  37,  38,  20,  56,  46,  65,  58,   8,  62,  10,  71,  17,\n",
       "         25,  28, 119,  48,  20, 104,  20,   7,  19, 140,  25,  61, 128,\n",
       "         94,  22,  49,  86,  45,  19,  80,  41,  32,  81,  92,  26,  29,\n",
       "         57,  18,  22,  16,  16,  28,  18,  83,  55,  15,  35,  18,  42,\n",
       "         25,  30,  53,  32,  69,  23,  27,  25,  45,  57, 102,  16,  46])>,\n",
       " <tf.Tensor: shape=(585,), dtype=int64, numpy=\n",
       " array([ 16,  12,  10,  17,  18,  15,   5,  27,   8,  18,  38,  29,  11,\n",
       "          6,  30,  54,  46,  75,  12,  10,  16,  13,  11,   5,  28,  13,\n",
       "         14,  14,  12,  32,  63,   7,  39,   8,  15,  18,  27,  54,  26,\n",
       "          5,   8,  10,  25,  21,  29,  16,  32,  17,  29,   8,  13,   9,\n",
       "         26,  13,  21,  17,  14,   7,  23,  31,  16,  16,  20,  14,  17,\n",
       "         51,  10,  27,  25,  15,  73,  16,  15,  34,  49,  33,  46,  19,\n",
       "         56,  26,  24,  21,  14,  41,  14,  19,   8,  22,  19,  14,  35,\n",
       "         15,  16,  17,  30,  41,  16,  36,  13,  28,  36,  16,  58,   8,\n",
       "         24,  64,  12,  11,  14,  10,  59,  51,  21,  15,   6,  13,  10,\n",
       "         10,  15,  29,  15,  86,  15,  27,  33,  43,  22,  18,  16,  10,\n",
       "         36,  12,  19,  48,  31,  27,   6,  46,  71,  11,  24,  24,   5,\n",
       "         22,  73,  19,  29,  58,  18,  17,  18,  30,  11,  59,  29,  42,\n",
       "         19,  19,  40,  15,  28,  21,   8,  13,  34,  48,   6,  18,  23,\n",
       "         27,  10,  34,  44,  53,  28,  32,  46,   5,  23,   9,  15,  25,\n",
       "         18,  12,  50,  14,  38,  29,  38,  15,  17,   9,  30,  16,  24,\n",
       "         13,  24,  26,  14,  10,  11,  40,  16,  17,  43,   8,  48,   6,\n",
       "         59,  13,  19,  15,  45,  10,   7,  21,  37,  11,  34,  16,  14,\n",
       "         41,   8,  13,  30,   9,   5,  11, 134,  30,  27,  42,  17,  85,\n",
       "         28,   7,  26,  38,  28,  41,  15,  32,  12,  59,  16,  46,   5,\n",
       "         22,  14,  65,  18,  25,   7,  17,  12,  54,  25,  12,  14,  15,\n",
       "          5,  18,   8,  27,  17,  18,   9,  14,  15,   9,  20,  35,  17,\n",
       "         45,  13,  37,  21,  14,  24,  28,  16,  22,  12,  19,   9,  33,\n",
       "         13,  29,   8,  19,  83,  28,  15,  18,  54,  25,  41,   8,  13,\n",
       "         29,  16,  15,  17,  61,  13,  28,   9,  15,  18,  35,   6,  16,\n",
       "         43,  16,  20,  24,  43,  16,  19,  16,   9, 123,  11,  17,  43,\n",
       "         17,  33,  22,  17,  29,  11,  13,  27,  22,  90,   5,  31,   7,\n",
       "         20,  47,  10,  62,  34,   9,  28,  16,  17,  16,  21,  15,  15,\n",
       "         22,  17,  13,  29,  60,  23,  28,  12,  29,  19,  22,  15,  25,\n",
       "         26,  30,  29,  58,  22,  11,  35,  41,  12,  25,  21,  24,  33,\n",
       "         27,  12,  17,  16,  20,   9,  10,  10,  46,  18,  42,  21,  49,\n",
       "         13,  23,  12,  16,  20,  21,  64,  50,  19,  17,   9,  11,  94,\n",
       "         16,  16,  41,  12,  13,  51,  75,  19,   8,   8,  30,  15,  23,\n",
       "         45,  11,  16,  40,  22,  33,  10,  58,  53,  88,  15,  27,  59,\n",
       "         44,   6,  16,   5,  13,  11,  16,  20,  13,  73,  14,  21,  12,\n",
       "         12,  61,  15,  13,  15,  19,  12,  20,  13,  89,  10,  39,  26,\n",
       "         16,  11,   8,  10,  19,  20,  19,   5,  88,  65,   5,  20,   7,\n",
       "         32,  15,  19,  32,  11,  43,  27,  20,  32,  11,  14,  24,  13,\n",
       "         10,  12,  31,  15,  18,  42,  13,  50,  75,  37,  12,  31,  11,\n",
       "         29,  18,  33,  19,  32,  14,  13,  27,   7,  25,  11,  11,  11,\n",
       "         18,  16,   7,   9,  16,  14,  20,   8,  26,  26,  23,  18,  20,\n",
       "         72,  16,  15,  14,  18,  21,  36,  35,   4,  24,   7,  25,  11,\n",
       "         15,  19,  50,  33,  13,  55,  12,   4,  12,  75,  13,  28,  75,\n",
       "         47,  14,  19,  30,  21,  13,  29,  22,  18,  36,  39,  15,  16,\n",
       "         27,  10,  11,   8,  10,  21,  15,  34,  28,   7,  16,   9,  17,\n",
       "         11,  16,  41,  16,  31,  12,  15,  12,  26,  30,  49,  16,  18])>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd13a516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_lengths = np.concatenate(lengths)\n",
    "\n",
    "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "max_length = max(all_lengths)\n",
    "plt.plot([max_length, max_length], plt.ylim())\n",
    "plt.title(f'Max tokens per example: {max_length}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa415c85",
   "metadata": {},
   "source": [
    "### Setting Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa32af",
   "metadata": {},
   "source": [
    "*Defining max_tokens*<br>\n",
    ">max_tokens: Here max num of words for each batch size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33a948f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76c13dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(pt, en):\n",
    "    #here pt and en are pt_examples[i] and en_examples[i]\n",
    "    pt = tokenizers.pt.tokenize(pt)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    pt = pt.to_tensor()\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return pt, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02c4f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt\n",
      " [[  2  88 120 ...   0   0   0]\n",
      " [  2 175 153 ...   0   0   0]\n",
      " [  2 101 105 ...   0   0   0]\n",
      " ...\n",
      " [  2 103 770 ...   0   0   0]\n",
      " [  2 133  14 ...   0   0   0]\n",
      " [  2  91 301 ...   0   0   0]] \n",
      "\n",
      " en\n",
      "\n",
      " [[  2 110  13 ...   0   0   0]\n",
      " [  2  45 628 ...   0   0   0]\n",
      " [  2  87 140 ...   0   0   0]\n",
      " ...\n",
      " [  2  77  71 ...   0   0   0]\n",
      " [  2 110  13 ...   0   0   0]\n",
      " [  2  99 474 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "## example\n",
    "##pt_example, and en_example are from row number \"Downloading Dataset 51 number\"\n",
    "pt,en=tokenize_pairs(pt_examples,en_examples)\n",
    "print('pt\\n {} \\n\\n en\\n\\n {}'.format(pt,en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7faad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt.shape: (585, 147)\n",
      "en.shape: (585, 134)\n"
     ]
    }
   ],
   "source": [
    "print('pt.shape: {}\\nen.shape: {}'.format(pt.shape,en.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5f81b",
   "metadata": {},
   "source": [
    "*bool function to filter out inputs with num_tokens*\n",
    "<br>tokenize pair are feeded to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "88e6a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_tokens(pt, en):\n",
    "    ##Here pt and en are tokenize value of pt_examples and en_examples\n",
    "    num_tokens = tf.maximum(tf.shape(pt)[1],tf.shape(en)[1])\n",
    "    return num_tokens < MAX_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14661fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(False, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "### Example of above function \n",
    "bool_tensor=filter_max_tokens(pt,en)\n",
    "print(bool_tensor)\n",
    "## meaning num_tokens is not less than the max tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbaf5e",
   "metadata": {},
   "source": [
    "**Example of tf.maximum**\n",
    "<code>\n",
    "    x = tf.constant([-5., -1., 0., 0.])\n",
    "    y = tf.constant([-3.])\n",
    "    c=tf.math.maximum(x, y)\n",
    "    print(c)\n",
    "</code>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ee05b",
   "metadata": {},
   "source": [
    "Here's a simple input pipeline that processes, shuffles and batches the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b5ad4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca7d77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)## Define above\n",
    "      .filter(filter_max_tokens) #Define above\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcc216",
   "metadata": {},
   "source": [
    "### Positional Encoding \n",
    ">Similar to Embedding done in tf.keras.layers.Embedding() <br>\n",
    ">>look into *An Attention is all we need* paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dddd48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60a5d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f36c66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20, 36), dtype=float32, numpy=\n",
       "array([[[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "        [ 8.41470957e-01,  5.40302277e-01,  5.64216733e-01,\n",
       "          8.25626731e-01,  3.51695180e-01,  9.36114550e-01,\n",
       "          2.13780671e-01,  9.76881683e-01,  1.28796190e-01,\n",
       "          9.91671085e-01,  7.73490295e-02,  9.97004092e-01,\n",
       "          4.63992245e-02,  9.98923004e-01,  2.78220028e-02,\n",
       "          9.99612868e-01,  1.66802313e-02,  9.99860883e-01,\n",
       "          9.99983307e-03,  9.99949992e-01,  5.99480653e-03,\n",
       "          9.99982059e-01,  3.59380594e-03,  9.99993563e-01,\n",
       "          2.15443294e-03,  9.99997675e-01,  1.29154930e-03,\n",
       "          9.99999166e-01,  7.74263579e-04,  9.99999702e-01,\n",
       "          4.64158860e-04,  9.99999881e-01,  2.78255931e-04,\n",
       "          9.99999940e-01,  1.66810059e-04,  1.00000000e+00],\n",
       "        [ 9.09297407e-01, -4.16146845e-01,  9.31664824e-01,\n",
       "          3.63318950e-01,  6.58454001e-01,  7.52620995e-01,\n",
       "          4.17676836e-01,  9.08595681e-01,  2.55446911e-01,\n",
       "          9.66823101e-01,  1.54234603e-01,  9.88034248e-01,\n",
       "          9.26984996e-02,  9.95694220e-01,  5.56224659e-02,\n",
       "          9.98451889e-01,  3.33558209e-02,  9.99443531e-01,\n",
       "          1.99986659e-02,  9.99800026e-01,  1.19893979e-02,\n",
       "          9.99928117e-01,  7.18756532e-03,  9.99974191e-01,\n",
       "          4.30885609e-03,  9.99990702e-01,  2.58309650e-03,\n",
       "          9.99996662e-01,  1.54852669e-03,  9.99998808e-01,\n",
       "          9.28317662e-04,  9.99999583e-01,  5.56511863e-04,\n",
       "          9.99999821e-01,  3.33620090e-04,  9.99999940e-01],\n",
       "        [ 1.41120002e-01, -9.89992499e-01,  9.74197984e-01,\n",
       "         -2.25695044e-01,  8.81081522e-01,  4.72964376e-01,\n",
       "          6.02261007e-01,  7.98299193e-01,  3.77842456e-01,\n",
       "          9.25869882e-01,  2.30196014e-01,  9.73144293e-01,\n",
       "          1.38798103e-01,  9.90320683e-01,  8.33798647e-02,\n",
       "          9.96517837e-01,  5.00221327e-02,  9.98748124e-01,\n",
       "          2.99955010e-02,  9.99550045e-01,  1.79835577e-02,\n",
       "          9.99838293e-01,  1.07812323e-02,  9.99941885e-01,\n",
       "          6.46325899e-03,  9.99979138e-01,  3.87463928e-03,\n",
       "          9.99992490e-01,  2.32278905e-03,  9.99997318e-01,\n",
       "          1.39247614e-03,  9.99999046e-01,  8.34767707e-04,\n",
       "          9.99999642e-01,  5.00430120e-04,  9.99999881e-01],\n",
       "        [-7.56802499e-01, -6.53643608e-01,  6.76982999e-01,\n",
       "         -7.35998690e-01,  9.91132557e-01,  1.32876709e-01,\n",
       "          7.58998692e-01,  6.51092112e-01,  4.93943959e-01,\n",
       "          8.69493723e-01,  3.04778129e-01,  9.52423394e-01,\n",
       "          1.84598729e-01,  9.82813954e-01,  1.11072712e-01,\n",
       "          9.93812263e-01,  6.66745231e-02,  9.97774780e-01,\n",
       "          3.99893336e-02,  9.99200106e-01,  2.39770729e-02,\n",
       "          9.99712527e-01,  1.43747600e-02,  9.99896705e-01,\n",
       "          8.61763209e-03,  9.99962866e-01,  5.16617578e-03,\n",
       "          9.99986649e-01,  3.09704989e-03,  9.99995232e-01,\n",
       "          1.85663451e-03,  9.99998271e-01,  1.11302349e-03,\n",
       "          9.99999404e-01,  6.67240180e-04,  9.99999762e-01],\n",
       "        [-9.58924294e-01,  2.83662200e-01,  1.43672481e-01,\n",
       "         -9.89625275e-01,  9.74545717e-01, -2.24188745e-01,\n",
       "          8.80642831e-01,  4.73780721e-01,  6.01817429e-01,\n",
       "          7.98633695e-01,  3.77534062e-01,  9.25995708e-01,\n",
       "          2.30001718e-01,  9.73190248e-01,  1.38679564e-01,\n",
       "          9.90337312e-01,  8.33083615e-02,  9.96523798e-01,\n",
       "          4.99791689e-02,  9.98750269e-01,  2.99697239e-02,\n",
       "          9.99550819e-01,  1.79681014e-02,  9.99838531e-01,\n",
       "          1.07719647e-02,  9.99942005e-01,  6.45770365e-03,\n",
       "          9.99979138e-01,  3.87130864e-03,  9.99992490e-01,\n",
       "          2.32079229e-03,  9.99997318e-01,  1.39127928e-03,\n",
       "          9.99999046e-01,  8.34050181e-04,  9.99999642e-01],\n",
       "        [-2.79415488e-01,  9.60170269e-01, -4.39743310e-01,\n",
       "         -8.98123503e-01,  8.33440363e-01, -5.52609384e-01,\n",
       "          9.61569011e-01,  2.74563283e-01,  6.99665904e-01,\n",
       "          7.14470148e-01,  4.48027879e-01,  8.94019604e-01,\n",
       "          2.74909258e-01,  9.61470187e-01,  1.66179046e-01,\n",
       "          9.86095607e-01,  9.99190211e-02,  9.94995594e-01,\n",
       "          5.99640049e-02,  9.98200536e-01,  3.59613001e-02,\n",
       "          9.99353170e-01,  2.15612110e-02,  9.99767542e-01,\n",
       "          1.29262479e-02,  9.99916434e-01,  7.74922036e-03,\n",
       "          9.99969959e-01,  4.64556552e-03,  9.99989212e-01,\n",
       "          2.78494973e-03,  9.99996126e-01,  1.66953483e-03,\n",
       "          9.99998629e-01,  1.00086012e-03,  9.99999523e-01],\n",
       "        [ 6.56986594e-01,  7.53902256e-01, -8.69800150e-01,\n",
       "         -4.93404210e-01,  5.85845590e-01, -8.10422659e-01,\n",
       "          9.98035491e-01,  6.26509860e-02,  7.85859525e-01,\n",
       "          6.18405104e-01,  5.15837193e-01,  8.56686652e-01,\n",
       "          3.19224656e-01,  9.47679043e-01,  1.93549871e-01,\n",
       "          9.81090426e-01,  1.16501875e-01,  9.93190467e-01,\n",
       "          6.99428469e-02,  9.97551024e-01,  4.19515818e-02,\n",
       "          9.99119639e-01,  2.51540430e-02,  9.99683559e-01,\n",
       "          1.50804715e-02,  9.99886274e-01,  9.04072449e-03,\n",
       "          9.99959111e-01,  5.41981915e-03,  9.99985337e-01,\n",
       "          3.24910646e-03,  9.99994695e-01,  1.94779038e-03,\n",
       "          9.99998093e-01,  1.16767013e-03,  9.99999344e-01],\n",
       "        [ 9.89358246e-01, -1.45500034e-01, -9.96517122e-01,\n",
       "          8.33880752e-02,  2.63396859e-01, -9.64687586e-01,\n",
       "          9.88356173e-01, -1.52158096e-01,  8.58962357e-01,\n",
       "          5.12038708e-01,  5.80555618e-01,  8.14220548e-01,\n",
       "          3.62852424e-01,  9.31846619e-01,  2.20770851e-01,\n",
       "          9.75325704e-01,  1.33052319e-01,  9.91109014e-01,\n",
       "          7.99146965e-02,  9.96801734e-01,  4.79403585e-02,\n",
       "          9.98850226e-01,  2.87465490e-02,  9.99586761e-01,\n",
       "          1.72346234e-02,  9.99851465e-01,  1.03322137e-02,\n",
       "          9.99946594e-01,  6.19406998e-03,  9.99980807e-01,\n",
       "          3.71326250e-03,  9.99993086e-01,  2.22604559e-03,\n",
       "          9.99997497e-01,  1.33448001e-03,  9.99999106e-01],\n",
       "        [ 4.12118495e-01, -9.11130250e-01, -7.75702238e-01,\n",
       "          6.31099045e-01, -9.27063376e-02, -9.95693505e-01,\n",
       "          9.32978570e-01, -3.59931886e-01,  9.17756796e-01,\n",
       "          3.97142917e-01,  6.41795516e-01,  7.66875803e-01,\n",
       "          4.05698568e-01,  9.14006948e-01,  2.47820899e-01,\n",
       "          9.68805850e-01,  1.49565727e-01,  9.88751769e-01,\n",
       "          8.98785517e-02,  9.95952725e-01,  5.39274104e-02,\n",
       "          9.98544872e-01,  3.23386826e-02,  9.99476969e-01,\n",
       "          1.93886980e-02,  9.99812007e-01,  1.16236852e-02,\n",
       "          9.99932468e-01,  6.96831662e-03,  9.99975741e-01,\n",
       "          4.17741761e-03,  9.99991298e-01,  2.50430079e-03,\n",
       "          9.99996841e-01,  1.50128989e-03,  9.99998868e-01],\n",
       "        [-5.44021130e-01, -8.39071512e-01, -2.84363836e-01,\n",
       "          9.58716452e-01, -4.36964363e-01, -8.99478793e-01,\n",
       "          8.34463179e-01, -5.51063657e-01,  9.61263359e-01,\n",
       "          2.75631577e-01,  6.99189842e-01,  7.14936078e-01,\n",
       "          4.47670847e-01,  8.94198418e-01,  2.74679095e-01,\n",
       "          9.61535931e-01,  1.66037530e-01,  9.86119449e-01,\n",
       "          9.98334140e-02,  9.95004177e-01,  5.99125251e-02,\n",
       "          9.98203635e-01,  3.59304026e-02,  9.99354303e-01,\n",
       "          2.15426795e-02,  9.99767959e-01,  1.29151372e-02,\n",
       "          9.99916613e-01,  7.74255954e-03,  9.99970019e-01,\n",
       "          4.64157201e-03,  9.99989212e-01,  2.78255576e-03,\n",
       "          9.99996126e-01,  1.66809978e-03,  9.99998629e-01],\n",
       "        [-9.99990225e-01,  4.42569796e-03,  3.06145489e-01,\n",
       "          9.51984763e-01, -7.25391090e-01, -6.88336968e-01,\n",
       "          6.97365046e-01, -7.16716111e-01,  9.88757372e-01,\n",
       "          1.49528801e-01,  7.52394736e-01,  6.58712506e-01,\n",
       "          4.88678783e-01,  8.72463763e-01,  3.01324606e-01,\n",
       "          9.53521609e-01,  1.82463139e-01,  9.83212709e-01,\n",
       "          1.09778300e-01,  9.93956089e-01,  6.58954829e-02,\n",
       "          9.97826517e-01,  3.95216532e-02,  9.99218702e-01,\n",
       "          2.36965641e-02,  9.99719203e-01,  1.42065687e-02,\n",
       "          9.99899089e-01,  8.51679780e-03,  9.99963760e-01,\n",
       "          5.10572549e-03,  9.99986947e-01,  3.06081050e-03,\n",
       "          9.99995291e-01,  1.83490955e-03,  9.99998331e-01],\n",
       "        [-5.36572933e-01,  8.43853951e-01,  7.89887607e-01,\n",
       "          6.13251626e-01, -9.21133935e-01, -3.89245719e-01,\n",
       "          5.28023124e-01, -8.49229991e-01,  9.99780834e-01,\n",
       "          2.09351983e-02,  8.01091373e-01,  5.98542035e-01,\n",
       "          5.28634131e-01,  8.48849773e-01,  3.27736855e-01,\n",
       "          9.44769025e-01,  1.98837966e-01,  9.80032384e-01,\n",
       "          1.19712204e-01,  9.92808640e-01,  7.18760788e-02,\n",
       "          9.97413576e-01,  4.31123972e-02,  9.99070227e-01,\n",
       "          2.58503370e-02,  9.99665797e-01,  1.54979751e-02,\n",
       "          9.99879897e-01,  9.29103047e-03,  9.99956846e-01,\n",
       "          5.56987757e-03,  9.99984503e-01,  3.33906501e-03,\n",
       "          9.99994397e-01,  2.00171932e-03,  9.99997973e-01],\n",
       "        [ 4.20167029e-01,  9.07446802e-01,  9.98159170e-01,\n",
       "          6.06491379e-02, -9.99182761e-01, -4.04202044e-02,\n",
       "          3.34267169e-01, -9.42478359e-01,  9.94150102e-01,\n",
       "         -1.08007133e-01,  8.44988048e-01,  5.34785211e-01,\n",
       "          5.67450762e-01,  8.23407352e-01,  3.53895366e-01,\n",
       "          9.35285032e-01,  2.15157464e-01,  9.76579368e-01,\n",
       "          1.29634142e-01,  9.91561890e-01,  7.78540894e-02,\n",
       "          9.96964753e-01,  4.67025824e-02,  9.98908818e-01,\n",
       "          2.80039888e-02,  9.99607801e-01,  1.67893562e-02,\n",
       "          9.99859035e-01,  1.00652575e-02,  9.99949336e-01,\n",
       "          6.03402872e-03,  9.99981821e-01,  3.61731928e-03,\n",
       "          9.99993443e-01,  2.16852897e-03,  9.99997675e-01],\n",
       "        [ 9.90607381e-01,  1.36737213e-01,  8.58326137e-01,\n",
       "         -5.13104558e-01, -9.49565172e-01,  3.13569844e-01,\n",
       "          1.25055820e-01, -9.92149711e-01,  9.71958995e-01,\n",
       "         -2.35150307e-01,  8.83821607e-01,  4.67824042e-01,\n",
       "          6.05045021e-01,  7.96191216e-01,  3.79779875e-01,\n",
       "          9.25076902e-01,  2.31417105e-01,  9.72854614e-01,\n",
       "          1.39543116e-01,  9.90216017e-01,  8.38292986e-02,\n",
       "          9.96480107e-01,  5.02921678e-02,  9.98734534e-01,\n",
       "          3.01575121e-02,  9.99545157e-01,  1.80807095e-02,\n",
       "          9.99836504e-01,  1.08394790e-02,  9.99941230e-01,\n",
       "          6.49817847e-03,  9.99978900e-01,  3.89557332e-03,\n",
       "          9.99992430e-01,  2.33533862e-03,  9.99997258e-01],\n",
       "        [ 6.50287867e-01, -7.59687901e-01,  4.19154823e-01,\n",
       "         -9.07914758e-01, -7.78620780e-01,  6.27494752e-01,\n",
       "         -8.99376869e-02, -9.95947421e-01,  9.33577180e-01,\n",
       "         -3.58376384e-01,  9.17359531e-01,  3.98059726e-01,\n",
       "          6.41336024e-01,  7.67260134e-01,  4.05370325e-01,\n",
       "          9.14152563e-01,  2.47612342e-01,  9.68859196e-01,\n",
       "          1.49438128e-01,  9.88771081e-01,  8.98014978e-02,\n",
       "          9.95959699e-01,  5.38811013e-02,  9.98547375e-01,\n",
       "          3.23108956e-02,  9.99477863e-01,  1.93720330e-02,\n",
       "          9.99812365e-01,  1.16136940e-02,  9.99932587e-01,\n",
       "          6.96232682e-03,  9.99975741e-01,  4.17382689e-03,\n",
       "          9.99991298e-01,  2.50214827e-03,  9.99996841e-01],\n",
       "        [-2.87903309e-01, -9.57659483e-01, -1.66195303e-01,\n",
       "         -9.86092865e-01, -5.08191347e-01,  8.61244202e-01,\n",
       "         -3.00772786e-01, -9.53695834e-01,  8.79643977e-01,\n",
       "         -4.75632668e-01,  9.45400715e-01,  3.25910300e-01,\n",
       "          6.76245570e-01,  7.36676276e-01,  4.30646986e-01,\n",
       "          9.02520478e-01,  2.63738692e-01,  9.64594185e-01,\n",
       "          1.59318209e-01,  9.87227261e-01,  9.57704708e-02,\n",
       "          9.95403469e-01,  5.74693382e-02,  9.98347282e-01,\n",
       "          3.44641283e-02,  9.99405921e-01,  2.06633247e-02,\n",
       "          9.99786496e-01,  1.23879025e-02,  9.99923289e-01,\n",
       "          7.42647378e-03,  9.99972403e-01,  4.45208047e-03,\n",
       "          9.99990106e-01,  2.66895769e-03,  9.99996424e-01],\n",
       "        [-9.61397469e-01, -2.75163352e-01, -6.93585396e-01,\n",
       "         -7.20374465e-01, -1.72829896e-01,  9.84951675e-01,\n",
       "         -4.97701138e-01, -8.67348611e-01,  8.11057806e-01,\n",
       "         -5.84965944e-01,  9.67777193e-01,  2.51808077e-01,\n",
       "          7.09698439e-01,  7.04505563e-01,  4.55590189e-01,\n",
       "          8.90189648e-01,  2.79791653e-01,  9.60060716e-01,\n",
       "          1.69182345e-01,  9.85584795e-01,  1.01736002e-01,\n",
       "          9.94811416e-01,  6.10568337e-02,  9.98134315e-01,\n",
       "          3.66172008e-02,  9.99329388e-01,  2.19545811e-02,\n",
       "          9.99758959e-01,  1.31621026e-02,  9.99913394e-01,\n",
       "          7.89061934e-03,  9.99968886e-01,  4.73033357e-03,\n",
       "          9.99988794e-01,  2.83576711e-03,  9.99996006e-01],\n",
       "        [-7.50987232e-01,  6.60316706e-01, -9.79089916e-01,\n",
       "         -2.03427911e-01,  1.84614182e-01,  9.82811093e-01,\n",
       "         -6.71617508e-01, -7.40898073e-01,  7.28961229e-01,\n",
       "         -6.84554994e-01,  9.84354913e-01,  1.76197037e-01,\n",
       "          7.41622627e-01,  6.70817316e-01,  4.80180681e-01,\n",
       "          8.77169609e-01,  2.95766771e-01,  9.55260158e-01,\n",
       "          1.79029569e-01,  9.83843684e-01,  1.07697874e-01,\n",
       "          9.94183660e-01,  6.46435395e-02,  9.97908413e-01,\n",
       "          3.87701057e-02,  9.99248147e-01,  2.32458003e-02,\n",
       "          9.99729753e-01,  1.39362952e-02,  9.99902904e-01,\n",
       "          8.35476257e-03,  9.99965072e-01,  5.00858575e-03,\n",
       "          9.99987483e-01,  3.00257653e-03,  9.99995470e-01],\n",
       "        [ 1.49877205e-01,  9.88704622e-01, -9.23140228e-01,\n",
       "          3.84463400e-01,  5.18469930e-01,  8.55095863e-01,\n",
       "         -8.14480484e-01, -5.80190897e-01,  6.34721696e-01,\n",
       "         -7.72740841e-01,  9.95034516e-01,  9.95302647e-02,\n",
       "          7.71949291e-01,  6.35684133e-01,  5.04399419e-01,\n",
       "          8.63470435e-01,  3.11659575e-01,  9.50193822e-01,\n",
       "          1.88858896e-01,  9.82004225e-01,  1.13655880e-01,\n",
       "          9.93520200e-01,  6.82294145e-02,  9.97669637e-01,\n",
       "          4.09228280e-02,  9.99162316e-01,  2.45369803e-02,\n",
       "          9.99698937e-01,  1.47104794e-02,  9.99891818e-01,\n",
       "          8.81890487e-03,  9.99961138e-01,  5.28683839e-03,\n",
       "          9.99986053e-01,  3.16938572e-03,  9.99994993e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(20,36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6412ce",
   "metadata": {},
   "source": [
    "### Masking\n",
    ">Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n",
    "\n",
    "*Note: We don't need masking skeleton based Posed based Action Recognition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d4070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b32943",
   "metadata": {},
   "source": [
    "*Similar function in array*\n",
    "<code>\n",
    "    def create_padding_mask(seq):\n",
    "        seq=np.where(seq!=0,0,1)\n",
    "        return seq\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2bd843",
   "metadata": {},
   "source": [
    "*Masking Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "267dfd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd658d",
   "metadata": {},
   "source": [
    "### look ahead mask\n",
    ">The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.<br>This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6065f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea1f6357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "\n",
    "temp = create_look_ahead_mask(x.shape[1]) \n",
    "#Value with 1 are masked \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02a921ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e4bd8",
   "metadata": {},
   "source": [
    "### Building Transfomrer from scratch\n",
    "i. Multi-Headed Attention Layer <br>\n",
    "1. Linear Layer <br>\n",
    "2. Scaler Dot Product Attention <br>\n",
    "3. Final Linear Layer<br> \n",
    "\n",
    "ii. Encoder <br>\n",
    "iii. Decoder <br>\n",
    "iv. Tranformer Model <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3efe9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    ## shape is determined by simple matrix multiplication rule \n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    #dk is the square root dimension of keys\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f754518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here d_model is feature dimension as well as dimension of the k\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads): ##(self,*,d_model=dimension of k,num_heads)\n",
    "        \n",
    "        ##  d_model refer to the dimension of the q,v,k\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model) ## Must Define dense layer separately for all because Dense() always expect same shape but q,v,k may have different shape\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "    #     Split the last dimension into (num_heads, depth).\n",
    "    #     Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "         #because of this split the computational time is same \n",
    "         #even when the number of heads is increased\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # q.shape= (batch_size, seq_len, d_model) ;Change the last dimesnion to d_model\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model) \n",
    "        ## this must assume the d_model and feature length is same \n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f88c16",
   "metadata": {},
   "source": [
    "*Testing the Above MultiHeadedAttention()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91a3a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([63, 43, 512]), TensorShape([63, 8, 43, 43]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((63,43,64))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ebacc06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([63, 43, 64])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcc4a6",
   "metadata": {},
   "source": [
    "### Point wise feed forward network\n",
    ">Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c989348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878d5d3",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "Each encoder layer consists of sublayers:\n",
    "1. Multi-head Attention\n",
    "2. Point wise feed forward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6ef7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # att_ouptput.shape=(batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        ##d_model must be equal to feature_dimension otherwise error in addition x + attn_output\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "22c6f116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4539e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        ## To embed to d_model\n",
    "        ## test code in below cell\n",
    "        self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)#\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        ## x.shape=[batch_size,seq_len]\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        ### We don't need this while using skeleton features\n",
    "        ### However the shape of the x will be [batch_size,seq_len,num_features]\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :] #Slices out the shape not in after embedding\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "11d6b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try the below code to understand more about embedding and positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea02f4a",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "emb=tf.keras.layers.Embedding(8500, 512)\n",
    "pos=positional_encoding(MAX_TOKENS,512)\n",
    "print('Pos_emb.shape: ',pos.shape)\n",
    "Pos_emb.shape:  (1, 128, 512)\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066220f3",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "x=tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200) #input\n",
    "x=emb(x)\n",
    "print('After Embedding: x.shape ',x.shape)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898ed2d",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "x *= tf.math.sqrt(tf.cast(512, tf.float32))\n",
    "print('x.shape: ',x.shape)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f31b05",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "pos=pos[:,:62,:] # 62 =tf.shape(x)[1] \n",
    "print('Pos_emb.shape: ',pos.shape)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdcdd87",
   "metadata": {},
   "source": [
    "<code>\n",
    "print(x.shape)=TensorShape([64, 62, 512])\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c7f77dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5fe2d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos_emb.shape:  (1, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "emb=tf.keras.layers.Embedding(8500, 512)\n",
    "pos=positional_encoding(MAX_TOKENS,512)\n",
    "print('Pos_emb.shape: ',pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ddb1a",
   "metadata": {},
   "source": [
    "## Decoder layer\n",
    "Each decoder layer consists of sublayers: <br>\n",
    "1. Masked multi-head attention (with look ahead mask and padding mask) \n",
    "2. *** Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.  ***\n",
    "3. Point wise feed forward networks.\n",
    "<br> <br>\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n",
    "\n",
    ">As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output.<b> In other words, the decoder predicts the next token by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ab3d1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) ## v,k,q order  \n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  ## v and k are from encoder \n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b95b0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "30779629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                   rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f3e32",
   "metadata": {},
   "source": [
    "### Creating a Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a095b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                   target_vocab_size, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               input_vocab_size=input_vocab_size, rate=rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               target_vocab_size=target_vocab_size, rate=rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return padding_mask, look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb963ac1",
   "metadata": {},
   "source": [
    "### Creating a sample Transformer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5aa2e6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=8500, target_vocab_size=8000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c3e333b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSummary of the model: \n",
      "\n",
      "Model: \"transformer_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_15 (Encoder)        multiple                  10656768  \n",
      "                                                                 \n",
      " decoder_9 (Decoder)         multiple                  12504064  \n",
      "                                                                 \n",
      " dense_683 (Dense)           multiple                  4104000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,264,832\n",
      "Trainable params: 27,264,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\tSummary of the model: \\n')\n",
    "print(sample_transformer.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af343b",
   "metadata": {},
   "source": [
    "### Setting the HyperParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "651dcb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a447b",
   "metadata": {},
   "source": [
    "### Test the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e93d86",
   "metadata": {},
   "source": [
    "### The main link has been updated. To find the previous turtorial on from tensorflow follow below link\n",
    "[previous_turtorial](https://github.com/tensorflow/text/blob/2.9/docs/tutorials/transformer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b9d1e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b714cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This learning rate is according to the paper \"Attention is all we need!\"\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5fedc524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8528e9",
   "metadata": {},
   "source": [
    "## Start from here next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2bf3d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "12cfab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "77629b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2bd247e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "238ac7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/train'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print('Latest checkpoint restored!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a7fa5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d9f9a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer([inp, tar_inp],\n",
    "                                 training = True)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "087d32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.8958 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.8254 Accuracy 0.0026\n",
      "Epoch 1 Batch 100 Loss 8.7114 Accuracy 0.0229\n",
      "Epoch 1 Batch 150 Loss 8.5964 Accuracy 0.0314\n",
      "Epoch 1 Batch 200 Loss 8.4578 Accuracy 0.0366\n",
      "Epoch 1 Batch 250 Loss 8.2897 Accuracy 0.0397\n",
      "Epoch 1 Batch 300 Loss 8.1001 Accuracy 0.0449\n",
      "Epoch 1 Batch 350 Loss 7.9007 Accuracy 0.0526\n",
      "Epoch 1 Batch 400 Loss 7.7109 Accuracy 0.0594\n",
      "Epoch 1 Batch 450 Loss 7.5410 Accuracy 0.0667\n",
      "Epoch 1 Batch 500 Loss 7.3958 Accuracy 0.0744\n",
      "Epoch 1 Batch 550 Loss 7.2599 Accuracy 0.0829\n",
      "Epoch 1 Batch 600 Loss 7.1314 Accuracy 0.0910\n",
      "Epoch 1 Batch 650 Loss 7.0121 Accuracy 0.0985\n",
      "Epoch 1 Loss 6.9097 Accuracy 0.1047\n",
      "Time taken for 1 epoch: 91.03 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 5.4292 Accuracy 0.1960\n",
      "Epoch 2 Batch 50 Loss 5.3838 Accuracy 0.1981\n",
      "Epoch 2 Batch 100 Loss 5.3439 Accuracy 0.2016\n",
      "Epoch 2 Batch 150 Loss 5.3115 Accuracy 0.2048\n",
      "Epoch 2 Batch 200 Loss 5.2714 Accuracy 0.2089\n",
      "Epoch 2 Batch 250 Loss 5.2436 Accuracy 0.2121\n",
      "Epoch 2 Batch 300 Loss 5.2131 Accuracy 0.2151\n",
      "Epoch 2 Batch 350 Loss 5.1871 Accuracy 0.2175\n",
      "Epoch 2 Batch 400 Loss 5.1593 Accuracy 0.2206\n",
      "Epoch 2 Batch 450 Loss 5.1356 Accuracy 0.2231\n",
      "Epoch 2 Batch 500 Loss 5.1117 Accuracy 0.2253\n",
      "Epoch 2 Batch 550 Loss 5.0926 Accuracy 0.2273\n",
      "Epoch 2 Batch 600 Loss 5.0679 Accuracy 0.2297\n",
      "Epoch 2 Batch 650 Loss 5.0464 Accuracy 0.2319\n",
      "Epoch 2 Loss 5.0265 Accuracy 0.2337\n",
      "Time taken for 1 epoch: 83.84 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 4.5561 Accuracy 0.2625\n",
      "Epoch 3 Batch 50 Loss 4.7172 Accuracy 0.2621\n",
      "Epoch 3 Batch 100 Loss 4.6906 Accuracy 0.2645\n",
      "Epoch 3 Batch 150 Loss 4.6793 Accuracy 0.2655\n",
      "Epoch 3 Batch 200 Loss 4.6714 Accuracy 0.2649\n",
      "Epoch 3 Batch 250 Loss 4.6581 Accuracy 0.2665\n",
      "Epoch 3 Batch 300 Loss 4.6469 Accuracy 0.2675\n",
      "Epoch 3 Batch 350 Loss 4.6338 Accuracy 0.2688\n",
      "Epoch 3 Batch 400 Loss 4.6197 Accuracy 0.2703\n",
      "Epoch 3 Batch 450 Loss 4.6064 Accuracy 0.2716\n",
      "Epoch 3 Batch 500 Loss 4.5918 Accuracy 0.2731\n",
      "Epoch 3 Batch 550 Loss 4.5784 Accuracy 0.2745\n",
      "Epoch 3 Batch 600 Loss 4.5641 Accuracy 0.2761\n",
      "Epoch 3 Batch 650 Loss 4.5501 Accuracy 0.2777\n",
      "Epoch 3 Batch 700 Loss 4.5352 Accuracy 0.2794\n",
      "Epoch 3 Loss 4.5347 Accuracy 0.2794\n",
      "Time taken for 1 epoch: 85.62 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 4.4035 Accuracy 0.2875\n",
      "Epoch 4 Batch 50 Loss 4.2321 Accuracy 0.3099\n",
      "Epoch 4 Batch 100 Loss 4.2320 Accuracy 0.3113\n",
      "Epoch 4 Batch 150 Loss 4.2123 Accuracy 0.3135\n",
      "Epoch 4 Batch 200 Loss 4.2002 Accuracy 0.3145\n",
      "Epoch 4 Batch 250 Loss 4.1870 Accuracy 0.3159\n",
      "Epoch 4 Batch 300 Loss 4.1712 Accuracy 0.3181\n",
      "Epoch 4 Batch 350 Loss 4.1565 Accuracy 0.3200\n",
      "Epoch 4 Batch 400 Loss 4.1384 Accuracy 0.3226\n",
      "Epoch 4 Batch 450 Loss 4.1216 Accuracy 0.3250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/akashghimire/28E0-A271/Python Projects/Transformer-Bert/BERT.ipynb Cell 96\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/akashghimire/28E0-A271/Python%20Projects/Transformer-Bert/BERT.ipynb#Y213sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# inp -> portuguese, tar -> english\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/akashghimire/28E0-A271/Python%20Projects/Transformer-Bert/BERT.ipynb#Y213sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m (batch, (inp, tar)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_batches):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/akashghimire/28E0-A271/Python%20Projects/Transformer-Bert/BERT.ipynb#Y213sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   train_step(inp, tar)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/akashghimire/28E0-A271/Python%20Projects/Transformer-Bert/BERT.ipynb#Y213sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/akashghimire/28E0-A271/Python%20Projects/Transformer-Bert/BERT.ipynb#Y213sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Batch \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Accuracy \u001b[39m\u001b[39m{\u001b[39;00mtrain_accuracy\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "    train_step(inp, tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49630df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c93af7433719cf61beb232a937287b5f6ac44c5a03632b389ba7312dbdbeed85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
