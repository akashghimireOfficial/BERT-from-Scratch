{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38fb8b9",
   "metadata": {},
   "source": [
    "## **BERT** \n",
    ">__[*original Turtorial Link*](https://www.tensorflow.org/text/tutorials/transformer)__\n",
    "\n",
    "### Turtorial Summary\n",
    "1. Downloading the Dataset\n",
    "2. Text tokenization & detokenization\n",
    "3. Setup of Input Pipeline \n",
    "4. Positional Encoding\n",
    "5. Buidling Transformer Model from Scracth\n",
    "6. Setting Hyperparameters \n",
    "7. Training and CheckPointing\n",
    "8. Attention Plot\n",
    "9. Exporting the saved model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffeb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d7cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b0ba5",
   "metadata": {},
   "source": [
    "#### Downloading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9555412",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading the Dataset\n",
    "## useing Tensorflow datasets\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017a9efe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "##if got time look more for what does the take() method refers to \n",
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "    for pt in pt_examples.numpy():\n",
    "        print(pt.decode('utf-8'))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dce6c3",
   "metadata": {},
   "source": [
    "### tokenization & detokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94c266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to train a NLP model first we need to tokenized them \n",
    "##Converting the text to sequence of token IDS \n",
    "## In simple words converting a sentence or words to list of unique tonek-IDS\n",
    "## for tekenization, in this turtorial we will be using frozen model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc11ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/ted_hrlr_translate_pt_en_converter.zip\n",
      "184801/184801 [==============================] - 0s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\\\ted_hrlr_translate_pt_en_converter.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.keras.utils.get_file(\n",
    "    f'{model_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c10c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading the model\n",
    "tokenizers=tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b40e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n't test for curiosity .\"], dtype=object)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56254ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Example of tokenization \n",
    "encoded_en_examples=tokenizers.en.tokenize(en_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a330803",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "## encoded tokens of above en_examples\n",
    "for row in encoded_en_examples.to_list():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43146852",
   "metadata": {},
   "source": [
    "*the shape of each input array is not same as the lenght of the sentence is not fixed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23c00f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "### Decoding the encoded english examples \n",
    "round_trip = tokenizers.en.detokenize(encoded_en_examples)\n",
    "for line in round_trip.numpy():\n",
    "    print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cba068",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How does the sequence of words are tokenized \n",
    "## using lookup() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa9492c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n",
      "  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n",
      "  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n",
      "  b'##ity', b'.', b'[END]']                                                 ,\n",
      " [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n",
      "  b'[END]']                                                           ,\n",
      " [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n",
      "  b'curiosity', b'.', b'[END]']                                          ]>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizers.en.lookup(encoded_en_examples)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076a03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................."
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for pt_examples, en_examples in train_examples.batch(1024):\n",
    "    pt_tokens = tokenizers.en.tokenize(pt_examples)\n",
    "    lengths.append(pt_tokens.row_lengths())\n",
    "    #print(pt_tokens.row_lengths())\n",
    "    en_tokens = tokenizers.en.tokenize(en_examples)\n",
    "    lengths.append(en_tokens.row_lengths())\n",
    "    print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9144f127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1024,), dtype=int64, numpy=array([45, 21, 23, ..., 21, 47, 35], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27,  9, 12, ...,  9, 23, 28], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 32,  22,  34, ...,  39, 101,  36], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17, 12, 20, ..., 30, 56, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([48, 37, 27, ..., 92, 21, 49], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([29, 18, 15, ..., 53, 13, 29], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([72, 38, 35, ..., 20, 54, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([38, 18, 16, ..., 11, 26, 11], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15,  24,  51, ...,  27,  40, 117], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 16, 24, ..., 20, 24, 71], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([28, 44, 26, ..., 95, 34, 32], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17, 28, 14, ..., 39, 23, 22], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 28, 43, ..., 71, 20, 18], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 15, 26, ..., 34,  7, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 32,  60,  25, ...,  37, 101,  31], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([18, 29, 15, ..., 22, 42, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 64, 26, ..., 38, 65, 41], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 28, 12, ..., 15, 25, 24], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([21, 74, 30, ..., 32, 84, 71], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([12, 40, 10, ..., 20, 30, 34], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([56, 54, 38, ..., 34, 22, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([45, 22, 21, ..., 14, 10, 16], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 30,  25,  56, ..., 114,  34,  75], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 14, 30, ..., 47, 17, 34], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([26, 24, 28, ..., 17, 29, 30], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([14, 10, 13, ..., 10, 13, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 25, 65, ..., 39, 56, 75], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 14, 33, ..., 15, 26, 39], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([48, 25, 23, ..., 65, 15,  8], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24,  9, 10, ..., 37, 10,  5], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([50, 35, 56, ..., 33, 23, 58], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([30, 19, 20, ..., 15, 11, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 52, 11, ..., 43, 25, 37], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 34, 10, ..., 25, 11, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([52, 83, 31, ..., 33, 98, 31], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([30, 38, 19, ..., 15, 34, 16], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 30,  14,  24, ..., 103,  17,  55], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17,  8, 34, ..., 51,  9, 37], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([36,  9,  8, ..., 55, 31, 37], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 10,  5, ..., 28, 13, 17], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 39,  34,  19, ...,  43, 115,  26], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 19, 10, ..., 29, 65,  9], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([51, 28, 57, ..., 48, 25, 41], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 16, 27, ..., 24, 12, 18], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([52, 32, 80, ..., 14, 62, 44], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 19, 42, ..., 11, 31, 20], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([34, 17, 49, ..., 12, 21, 25], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 12, 30, ...,  8, 11, 11], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 68, 119,  19, ...,  11,  21,  35], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([31, 54, 11, ..., 12, 12, 25], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([38, 25, 14, ..., 54, 21, 40], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([25, 10,  6, ..., 24, 10, 27], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 57,  41,  14, ...,  26,  92, 198], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 36,  20,  12, ...,  12,  40, 103], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([25, 30, 40, ..., 35, 36, 30], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 22, 21, ..., 22, 17, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 26, 23, ..., 98, 32, 42], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 16, 12, ..., 52, 14, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 65, 23, ..., 63, 47, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 9, 34, 15, ..., 40, 24, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([12, 21, 37, ..., 28, 16, 25], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10,  8, 16, ..., 11,  9, 12], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([99, 82, 41, ..., 33, 59, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([41, 46, 22, ..., 13, 25,  8], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15,  25,  64, ..., 158,  25, 103], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 14, 44, ..., 78, 13, 40], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([28, 13, 78, ..., 25, 39, 12], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19,  9, 31, ..., 12, 22, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([20,  9, 90, ..., 21, 66, 17], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11,  5, 34, ..., 12, 27, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([127,  28,  35, ...,  75,  43,  14], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([87, 16, 12, ..., 34, 18, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([  8,  27,  90, ...,  48,  10, 227], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([  5,  19,  38, ...,  23,   4, 130], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([65, 69, 37, ..., 13,  9, 11], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([32, 36, 19, ..., 13,  5, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 37, 21, ..., 49, 15, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([20, 17, 11, ..., 28, 13, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 24, 75, ..., 40,  9, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 8, 12, 53, ..., 21,  8, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([21, 55, 24, ..., 36, 34, 36], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 27, 10, ..., 24, 19, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([46, 38, 28, ..., 56, 53, 40], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 27, 11, ..., 26, 24, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 42, 241,  50, ...,  64,  40,  54], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15, 117,  23, ...,  31,  24,  21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 29, 29, ..., 49, 24, 63], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24,  8, 19, ..., 33, 13, 32], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([43, 27, 28, ..., 76, 51, 46], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([33, 13, 11, ..., 43, 24, 16], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 28, 21, ..., 21, 46, 45], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24, 16, 12, ..., 10, 23, 26], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([76, 26, 52, ..., 29, 53, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([40, 19, 23, ..., 19, 20, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([106,  15,  14, ...,  73,  25,  56], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([44,  9,  9, ..., 32, 18, 32], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([44, 30, 33, ..., 52, 42, 38], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([18, 13, 21, ..., 27, 16, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 31, 12, ..., 98, 19, 24], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 7, 19, 11, ..., 73,  9, 14], dtype=int64)>,\n",
       " <tf.Tensor: shape=(585,), dtype=int64, numpy=\n",
       " array([ 25,  22,  11,  26,  34,  34,   8,  63,  15,  37,  71,  67,  18,\n",
       "         11,  53, 109,  84, 143,  20,  17,  30,  30,  19,  10,  46,  30,\n",
       "         24,  23,  22,  60, 153,  19,  69,  20,  18,  20,  40, 113,  54,\n",
       "          9,  20,  15,  43,  48,  55,  34,  55,  38,  56,  14,  24,  20,\n",
       "         45,  28,  32,  26,  18,  15,  40,  57,  32,  28,  39,  13,  30,\n",
       "         89,  18,  48,  57,  40, 128,  25,  36,  70,  94,  71,  85,  29,\n",
       "        148,  48,  39,  50,  33, 111,  19,  31,  19,  45,  34,  29, 100,\n",
       "         30,  28,  25,  66,  76,  36,  67,  18,  55,  50,  35,  85,  17,\n",
       "         51, 130,  19,  14,  33,  23,  84, 108,  34,  24,   9,  23,  18,\n",
       "         17,  32,  67,  21, 135,  24,  53,  57,  78,  43,  30,  36,  22,\n",
       "         54,  24,  34,  62,  65,  63,  10,  87, 151,  16,  53,  41,   9,\n",
       "         45, 138,  29,  67, 122,  31,  33,  27,  53,  17, 138,  54,  81,\n",
       "         33,  35,  81,  32,  46,  29,  25,  13,  55,  92,   9,  29,  48,\n",
       "         54,  25,  75,  91, 109,  56,  63,  54,   9,  65,  15,  39,  39,\n",
       "         23,  23,  78,  21,  60,  66,  65,  30,  22,  27,  51,  29,  31,\n",
       "         27,  42,  49,  31,  19,  13,  71,  25,  23,  81,  12,  88,  16,\n",
       "         99,  18,  31,  20, 106,  21,  11,  29,  71,  21,  47,  20,  19,\n",
       "         72,  14,  34,  58,  18,   9,  15, 248,  67,  50,  94,  22, 150,\n",
       "         49,  14,  37,  80,  47,  89,  29,  60,  30,  98,  24,  91,   9,\n",
       "         44,  26, 131,  43,  50,  10,  40,  19,  87,  58,  20,  17,  27,\n",
       "          8,  35,  15,  67,  33,  28,  11,  39,  27,  18,  39,  45,  37,\n",
       "         86,  27, 106,  36,  26,  20,  53,  31,  41,  24,  24,  21,  51,\n",
       "         18,  55,  10,  29, 178,  65,  33,  24,  99,  44,  91,  18,  24,\n",
       "         41,  28,  18,  37,  83,  23,  73,  21,  27,  35,  87,   7,  33,\n",
       "         79,  28,  29,  42,  75,  31,  28,  26,  17, 211,  19,  31,  94,\n",
       "         36,  26,  43,  24,  50,  40,  20,  42,  39, 169,   8,  53,  18,\n",
       "         37,  84,  26,  90,  78,  18,  53,  25,  28,  40,  16,  36,  24,\n",
       "         44,  32,  22,  59, 140,  57,  62,  29,  49,  48,  45,  20,  45,\n",
       "         47,  47,  57, 136,  29,  18,  62,  70,  14,  45,  38,  49,  74,\n",
       "         37,  23,  16,  34,  38,  19,  18,  16,  88,  32,  78,  49,  88,\n",
       "         26,  44,  16,  31,  30,  52, 137, 114,  33,  26,  22,  24, 172,\n",
       "         32,  28,  90,  15,  27,  91, 108,  52,   9,  11,  45,  24,  37,\n",
       "         99,  19,  24, 107,  48,  70,  22,  85, 111, 178,  30,  38, 118,\n",
       "         85,  10,  33,   8,  22,  29,  19,  28,  21, 177,  24,  55,  30,\n",
       "         17, 135,  27,  22,  31,  34,  20,  33,  34, 142,  23,  75,  45,\n",
       "         32,  21,  21,  15,  31,  32,  30,  10, 158, 148,   9,  45,  19,\n",
       "         55,  24,  47,  66,  16,  86,  55,  48,  53,  19,  21,  39,  22,\n",
       "         15,  20,  84,  29,  38,  77,  28,  79, 187,  57,  23,  61,  23,\n",
       "         74,  41,  55,  32,  79,  32,  30,  54,   7,  50,  16,  21,  25,\n",
       "         51,  31,  14,  16,  51,  35,  36,  16,  50,  56,  59,  47,  34,\n",
       "        140,  37,  38,  20,  56,  46,  65,  58,   8,  62,  10,  71,  17,\n",
       "         25,  28, 119,  48,  20, 104,  20,   7,  19, 140,  25,  61, 128,\n",
       "         94,  22,  49,  86,  45,  19,  80,  41,  32,  81,  92,  26,  29,\n",
       "         57,  18,  22,  16,  16,  28,  18,  83,  55,  15,  35,  18,  42,\n",
       "         25,  30,  53,  32,  69,  23,  27,  25,  45,  57, 102,  16,  46],\n",
       "       dtype=int64)>,\n",
       " <tf.Tensor: shape=(585,), dtype=int64, numpy=\n",
       " array([ 16,  12,  10,  17,  18,  15,   5,  27,   8,  18,  38,  29,  11,\n",
       "          6,  30,  54,  46,  75,  12,  10,  16,  13,  11,   5,  28,  13,\n",
       "         14,  14,  12,  32,  63,   7,  39,   8,  15,  18,  27,  54,  26,\n",
       "          5,   8,  10,  25,  21,  29,  16,  32,  17,  29,   8,  13,   9,\n",
       "         26,  13,  21,  17,  14,   7,  23,  31,  16,  16,  20,  14,  17,\n",
       "         51,  10,  27,  25,  15,  73,  16,  15,  34,  49,  33,  46,  19,\n",
       "         56,  26,  24,  21,  14,  41,  14,  19,   8,  22,  19,  14,  35,\n",
       "         15,  16,  17,  30,  41,  16,  36,  13,  28,  36,  16,  58,   8,\n",
       "         24,  64,  12,  11,  14,  10,  59,  51,  21,  15,   6,  13,  10,\n",
       "         10,  15,  29,  15,  86,  15,  27,  33,  43,  22,  18,  16,  10,\n",
       "         36,  12,  19,  48,  31,  27,   6,  46,  71,  11,  24,  24,   5,\n",
       "         22,  73,  19,  29,  58,  18,  17,  18,  30,  11,  59,  29,  42,\n",
       "         19,  19,  40,  15,  28,  21,   8,  13,  34,  48,   6,  18,  23,\n",
       "         27,  10,  34,  44,  53,  28,  32,  46,   5,  23,   9,  15,  25,\n",
       "         18,  12,  50,  14,  38,  29,  38,  15,  17,   9,  30,  16,  24,\n",
       "         13,  24,  26,  14,  10,  11,  40,  16,  17,  43,   8,  48,   6,\n",
       "         59,  13,  19,  15,  45,  10,   7,  21,  37,  11,  34,  16,  14,\n",
       "         41,   8,  13,  30,   9,   5,  11, 134,  30,  27,  42,  17,  85,\n",
       "         28,   7,  26,  38,  28,  41,  15,  32,  12,  59,  16,  46,   5,\n",
       "         22,  14,  65,  18,  25,   7,  17,  12,  54,  25,  12,  14,  15,\n",
       "          5,  18,   8,  27,  17,  18,   9,  14,  15,   9,  20,  35,  17,\n",
       "         45,  13,  37,  21,  14,  24,  28,  16,  22,  12,  19,   9,  33,\n",
       "         13,  29,   8,  19,  83,  28,  15,  18,  54,  25,  41,   8,  13,\n",
       "         29,  16,  15,  17,  61,  13,  28,   9,  15,  18,  35,   6,  16,\n",
       "         43,  16,  20,  24,  43,  16,  19,  16,   9, 123,  11,  17,  43,\n",
       "         17,  33,  22,  17,  29,  11,  13,  27,  22,  90,   5,  31,   7,\n",
       "         20,  47,  10,  62,  34,   9,  28,  16,  17,  16,  21,  15,  15,\n",
       "         22,  17,  13,  29,  60,  23,  28,  12,  29,  19,  22,  15,  25,\n",
       "         26,  30,  29,  58,  22,  11,  35,  41,  12,  25,  21,  24,  33,\n",
       "         27,  12,  17,  16,  20,   9,  10,  10,  46,  18,  42,  21,  49,\n",
       "         13,  23,  12,  16,  20,  21,  64,  50,  19,  17,   9,  11,  94,\n",
       "         16,  16,  41,  12,  13,  51,  75,  19,   8,   8,  30,  15,  23,\n",
       "         45,  11,  16,  40,  22,  33,  10,  58,  53,  88,  15,  27,  59,\n",
       "         44,   6,  16,   5,  13,  11,  16,  20,  13,  73,  14,  21,  12,\n",
       "         12,  61,  15,  13,  15,  19,  12,  20,  13,  89,  10,  39,  26,\n",
       "         16,  11,   8,  10,  19,  20,  19,   5,  88,  65,   5,  20,   7,\n",
       "         32,  15,  19,  32,  11,  43,  27,  20,  32,  11,  14,  24,  13,\n",
       "         10,  12,  31,  15,  18,  42,  13,  50,  75,  37,  12,  31,  11,\n",
       "         29,  18,  33,  19,  32,  14,  13,  27,   7,  25,  11,  11,  11,\n",
       "         18,  16,   7,   9,  16,  14,  20,   8,  26,  26,  23,  18,  20,\n",
       "         72,  16,  15,  14,  18,  21,  36,  35,   4,  24,   7,  25,  11,\n",
       "         15,  19,  50,  33,  13,  55,  12,   4,  12,  75,  13,  28,  75,\n",
       "         47,  14,  19,  30,  21,  13,  29,  22,  18,  36,  39,  15,  16,\n",
       "         27,  10,  11,   8,  10,  21,  15,  34,  28,   7,  16,   9,  17,\n",
       "         11,  16,  41,  16,  31,  12,  15,  12,  26,  30,  49,  16,  18],\n",
       "       dtype=int64)>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd13a516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_lengths = np.concatenate(lengths)\n",
    "\n",
    "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "max_length = max(all_lengths)\n",
    "plt.plot([max_length, max_length], plt.ylim())\n",
    "plt.title(f'Max tokens per example: {max_length}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa415c85",
   "metadata": {},
   "source": [
    "### Setting Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa32af",
   "metadata": {},
   "source": [
    "*Defining max_tokens*<br>\n",
    ">max_tokens: Here max num of words for each batch size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33a948f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76c13dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(pt, en):\n",
    "    #here pt and en are pt_examples[i] and en_examples[i]\n",
    "    pt = tokenizers.pt.tokenize(pt)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    pt = pt.to_tensor()\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return pt, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dafc158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading the Dataset\n",
    "## useing Tensorflow datasets\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02c4f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt\n",
      " [[  2  88 120 ...   0   0   0]\n",
      " [  2 175 153 ...   0   0   0]\n",
      " [  2 101 105 ...   0   0   0]\n",
      " ...\n",
      " [  2 103 770 ...   0   0   0]\n",
      " [  2 133  14 ...   0   0   0]\n",
      " [  2  91 301 ...   0   0   0]] \n",
      "\n",
      " en\n",
      "\n",
      " [[  2 110  13 ...   0   0   0]\n",
      " [  2  45 628 ...   0   0   0]\n",
      " [  2  87 140 ...   0   0   0]\n",
      " ...\n",
      " [  2  77  71 ...   0   0   0]\n",
      " [  2 110  13 ...   0   0   0]\n",
      " [  2  99 474 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "## example\n",
    "##pt_example, and en_example are from row number \"Downloading Dataset 51 number\"\n",
    "pt,en=tokenize_pairs(pt_examples,en_examples)\n",
    "print('pt\\n {} \\n\\n en\\n\\n {}'.format(pt,en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7faad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt.shape: (585, 147)\n",
      "en.shape: (585, 134)\n"
     ]
    }
   ],
   "source": [
    "print('pt.shape: {}\\nen.shape: {}'.format(pt.shape,en.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5f81b",
   "metadata": {},
   "source": [
    "*bool function to filter out inputs with num_tokens*\n",
    "<br>tokenize pair are feeded to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88e6a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_tokens(pt, en):\n",
    "    ##Here pt and en are tokenize value of pt_examples and en_examples\n",
    "    num_tokens = tf.maximum(tf.shape(pt)[1],tf.shape(en)[1])\n",
    "    return num_tokens < MAX_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14661fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(False, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "### Example of above function \n",
    "bool_tensor=filter_max_tokens(pt,en)\n",
    "print(bool_tensor)\n",
    "## meaning num_tokens is not less than the max tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbaf5e",
   "metadata": {},
   "source": [
    "**Example of tf.maximum**\n",
    "<code>\n",
    "    x = tf.constant([-5., -1., 0., 0.])\n",
    "    y = tf.constant([-3.])\n",
    "    c=tf.math.maximum(x, y)\n",
    "    print(c)\n",
    "</code>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ee05b",
   "metadata": {},
   "source": [
    "Here's a simple input pipeline that processes, shuffles and batches the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b5ad4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca7d77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)## Define above\n",
    "      .filter(filter_max_tokens) #Define above\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcc216",
   "metadata": {},
   "source": [
    "### Positional Encoding \n",
    ">Similar to Embedding done in tf.keras.layers.Embedding() <br>\n",
    ">>look into *An Attention is all we need* paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dddd48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60a5d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f36c66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20, 36), dtype=float32, numpy=\n",
       "array([[[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "        [ 8.41470957e-01,  5.40302277e-01,  5.64216733e-01,\n",
       "          8.25626731e-01,  3.51695180e-01,  9.36114550e-01,\n",
       "          2.13780671e-01,  9.76881683e-01,  1.28796190e-01,\n",
       "          9.91671085e-01,  7.73490295e-02,  9.97004092e-01,\n",
       "          4.63992245e-02,  9.98923004e-01,  2.78220028e-02,\n",
       "          9.99612868e-01,  1.66802313e-02,  9.99860883e-01,\n",
       "          9.99983307e-03,  9.99949992e-01,  5.99480653e-03,\n",
       "          9.99982059e-01,  3.59380594e-03,  9.99993563e-01,\n",
       "          2.15443294e-03,  9.99997675e-01,  1.29154930e-03,\n",
       "          9.99999166e-01,  7.74263579e-04,  9.99999702e-01,\n",
       "          4.64158860e-04,  9.99999881e-01,  2.78255931e-04,\n",
       "          9.99999940e-01,  1.66810059e-04,  1.00000000e+00],\n",
       "        [ 9.09297407e-01, -4.16146845e-01,  9.31664824e-01,\n",
       "          3.63318950e-01,  6.58454001e-01,  7.52620995e-01,\n",
       "          4.17676836e-01,  9.08595681e-01,  2.55446911e-01,\n",
       "          9.66823101e-01,  1.54234603e-01,  9.88034248e-01,\n",
       "          9.26984996e-02,  9.95694220e-01,  5.56224659e-02,\n",
       "          9.98451889e-01,  3.33558209e-02,  9.99443531e-01,\n",
       "          1.99986659e-02,  9.99800026e-01,  1.19893979e-02,\n",
       "          9.99928117e-01,  7.18756532e-03,  9.99974191e-01,\n",
       "          4.30885609e-03,  9.99990702e-01,  2.58309650e-03,\n",
       "          9.99996662e-01,  1.54852669e-03,  9.99998808e-01,\n",
       "          9.28317662e-04,  9.99999583e-01,  5.56511863e-04,\n",
       "          9.99999821e-01,  3.33620090e-04,  9.99999940e-01],\n",
       "        [ 1.41120002e-01, -9.89992499e-01,  9.74197984e-01,\n",
       "         -2.25695044e-01,  8.81081522e-01,  4.72964376e-01,\n",
       "          6.02261007e-01,  7.98299193e-01,  3.77842456e-01,\n",
       "          9.25869882e-01,  2.30196014e-01,  9.73144293e-01,\n",
       "          1.38798103e-01,  9.90320683e-01,  8.33798647e-02,\n",
       "          9.96517837e-01,  5.00221327e-02,  9.98748124e-01,\n",
       "          2.99955010e-02,  9.99550045e-01,  1.79835577e-02,\n",
       "          9.99838293e-01,  1.07812323e-02,  9.99941885e-01,\n",
       "          6.46325899e-03,  9.99979138e-01,  3.87463928e-03,\n",
       "          9.99992490e-01,  2.32278905e-03,  9.99997318e-01,\n",
       "          1.39247614e-03,  9.99999046e-01,  8.34767707e-04,\n",
       "          9.99999642e-01,  5.00430120e-04,  9.99999881e-01],\n",
       "        [-7.56802499e-01, -6.53643608e-01,  6.76982999e-01,\n",
       "         -7.35998690e-01,  9.91132557e-01,  1.32876709e-01,\n",
       "          7.58998692e-01,  6.51092112e-01,  4.93943959e-01,\n",
       "          8.69493723e-01,  3.04778129e-01,  9.52423394e-01,\n",
       "          1.84598729e-01,  9.82813954e-01,  1.11072712e-01,\n",
       "          9.93812263e-01,  6.66745231e-02,  9.97774780e-01,\n",
       "          3.99893336e-02,  9.99200106e-01,  2.39770729e-02,\n",
       "          9.99712527e-01,  1.43747600e-02,  9.99896705e-01,\n",
       "          8.61763209e-03,  9.99962866e-01,  5.16617578e-03,\n",
       "          9.99986649e-01,  3.09704989e-03,  9.99995232e-01,\n",
       "          1.85663451e-03,  9.99998271e-01,  1.11302349e-03,\n",
       "          9.99999404e-01,  6.67240180e-04,  9.99999762e-01],\n",
       "        [-9.58924294e-01,  2.83662200e-01,  1.43672481e-01,\n",
       "         -9.89625275e-01,  9.74545717e-01, -2.24188745e-01,\n",
       "          8.80642831e-01,  4.73780721e-01,  6.01817429e-01,\n",
       "          7.98633695e-01,  3.77534062e-01,  9.25995708e-01,\n",
       "          2.30001718e-01,  9.73190248e-01,  1.38679564e-01,\n",
       "          9.90337312e-01,  8.33083615e-02,  9.96523798e-01,\n",
       "          4.99791689e-02,  9.98750269e-01,  2.99697239e-02,\n",
       "          9.99550819e-01,  1.79681014e-02,  9.99838531e-01,\n",
       "          1.07719647e-02,  9.99942005e-01,  6.45770365e-03,\n",
       "          9.99979138e-01,  3.87130864e-03,  9.99992490e-01,\n",
       "          2.32079229e-03,  9.99997318e-01,  1.39127928e-03,\n",
       "          9.99999046e-01,  8.34050181e-04,  9.99999642e-01],\n",
       "        [-2.79415488e-01,  9.60170269e-01, -4.39743310e-01,\n",
       "         -8.98123503e-01,  8.33440363e-01, -5.52609384e-01,\n",
       "          9.61569011e-01,  2.74563283e-01,  6.99665904e-01,\n",
       "          7.14470148e-01,  4.48027879e-01,  8.94019604e-01,\n",
       "          2.74909258e-01,  9.61470187e-01,  1.66179046e-01,\n",
       "          9.86095607e-01,  9.99190211e-02,  9.94995594e-01,\n",
       "          5.99640049e-02,  9.98200536e-01,  3.59613001e-02,\n",
       "          9.99353170e-01,  2.15612110e-02,  9.99767542e-01,\n",
       "          1.29262479e-02,  9.99916434e-01,  7.74922036e-03,\n",
       "          9.99969959e-01,  4.64556552e-03,  9.99989212e-01,\n",
       "          2.78494973e-03,  9.99996126e-01,  1.66953483e-03,\n",
       "          9.99998629e-01,  1.00086012e-03,  9.99999523e-01],\n",
       "        [ 6.56986594e-01,  7.53902256e-01, -8.69800150e-01,\n",
       "         -4.93404210e-01,  5.85845590e-01, -8.10422659e-01,\n",
       "          9.98035491e-01,  6.26509860e-02,  7.85859525e-01,\n",
       "          6.18405104e-01,  5.15837193e-01,  8.56686652e-01,\n",
       "          3.19224656e-01,  9.47679043e-01,  1.93549871e-01,\n",
       "          9.81090426e-01,  1.16501875e-01,  9.93190467e-01,\n",
       "          6.99428469e-02,  9.97551024e-01,  4.19515818e-02,\n",
       "          9.99119639e-01,  2.51540430e-02,  9.99683559e-01,\n",
       "          1.50804715e-02,  9.99886274e-01,  9.04072449e-03,\n",
       "          9.99959111e-01,  5.41981915e-03,  9.99985337e-01,\n",
       "          3.24910646e-03,  9.99994695e-01,  1.94779038e-03,\n",
       "          9.99998093e-01,  1.16767013e-03,  9.99999344e-01],\n",
       "        [ 9.89358246e-01, -1.45500034e-01, -9.96517122e-01,\n",
       "          8.33880752e-02,  2.63396859e-01, -9.64687586e-01,\n",
       "          9.88356173e-01, -1.52158096e-01,  8.58962357e-01,\n",
       "          5.12038708e-01,  5.80555618e-01,  8.14220548e-01,\n",
       "          3.62852424e-01,  9.31846619e-01,  2.20770851e-01,\n",
       "          9.75325704e-01,  1.33052319e-01,  9.91109014e-01,\n",
       "          7.99146965e-02,  9.96801734e-01,  4.79403585e-02,\n",
       "          9.98850226e-01,  2.87465490e-02,  9.99586761e-01,\n",
       "          1.72346234e-02,  9.99851465e-01,  1.03322137e-02,\n",
       "          9.99946594e-01,  6.19406998e-03,  9.99980807e-01,\n",
       "          3.71326250e-03,  9.99993086e-01,  2.22604559e-03,\n",
       "          9.99997497e-01,  1.33448001e-03,  9.99999106e-01],\n",
       "        [ 4.12118495e-01, -9.11130250e-01, -7.75702238e-01,\n",
       "          6.31099045e-01, -9.27063376e-02, -9.95693505e-01,\n",
       "          9.32978570e-01, -3.59931886e-01,  9.17756796e-01,\n",
       "          3.97142917e-01,  6.41795516e-01,  7.66875803e-01,\n",
       "          4.05698568e-01,  9.14006948e-01,  2.47820899e-01,\n",
       "          9.68805850e-01,  1.49565727e-01,  9.88751769e-01,\n",
       "          8.98785517e-02,  9.95952725e-01,  5.39274104e-02,\n",
       "          9.98544872e-01,  3.23386826e-02,  9.99476969e-01,\n",
       "          1.93886980e-02,  9.99812007e-01,  1.16236852e-02,\n",
       "          9.99932468e-01,  6.96831662e-03,  9.99975741e-01,\n",
       "          4.17741761e-03,  9.99991298e-01,  2.50430079e-03,\n",
       "          9.99996841e-01,  1.50128989e-03,  9.99998868e-01],\n",
       "        [-5.44021130e-01, -8.39071512e-01, -2.84363836e-01,\n",
       "          9.58716452e-01, -4.36964363e-01, -8.99478793e-01,\n",
       "          8.34463179e-01, -5.51063657e-01,  9.61263359e-01,\n",
       "          2.75631577e-01,  6.99189842e-01,  7.14936078e-01,\n",
       "          4.47670847e-01,  8.94198418e-01,  2.74679095e-01,\n",
       "          9.61535931e-01,  1.66037530e-01,  9.86119449e-01,\n",
       "          9.98334140e-02,  9.95004177e-01,  5.99125251e-02,\n",
       "          9.98203635e-01,  3.59304026e-02,  9.99354303e-01,\n",
       "          2.15426795e-02,  9.99767959e-01,  1.29151372e-02,\n",
       "          9.99916613e-01,  7.74255954e-03,  9.99970019e-01,\n",
       "          4.64157201e-03,  9.99989212e-01,  2.78255576e-03,\n",
       "          9.99996126e-01,  1.66809978e-03,  9.99998629e-01],\n",
       "        [-9.99990225e-01,  4.42569796e-03,  3.06145489e-01,\n",
       "          9.51984763e-01, -7.25391090e-01, -6.88336968e-01,\n",
       "          6.97365046e-01, -7.16716111e-01,  9.88757372e-01,\n",
       "          1.49528801e-01,  7.52394736e-01,  6.58712506e-01,\n",
       "          4.88678783e-01,  8.72463763e-01,  3.01324606e-01,\n",
       "          9.53521609e-01,  1.82463139e-01,  9.83212709e-01,\n",
       "          1.09778300e-01,  9.93956089e-01,  6.58954829e-02,\n",
       "          9.97826517e-01,  3.95216532e-02,  9.99218702e-01,\n",
       "          2.36965641e-02,  9.99719203e-01,  1.42065687e-02,\n",
       "          9.99899089e-01,  8.51679780e-03,  9.99963760e-01,\n",
       "          5.10572549e-03,  9.99986947e-01,  3.06081050e-03,\n",
       "          9.99995291e-01,  1.83490955e-03,  9.99998331e-01],\n",
       "        [-5.36572933e-01,  8.43853951e-01,  7.89887607e-01,\n",
       "          6.13251626e-01, -9.21133935e-01, -3.89245719e-01,\n",
       "          5.28023124e-01, -8.49229991e-01,  9.99780834e-01,\n",
       "          2.09351983e-02,  8.01091373e-01,  5.98542035e-01,\n",
       "          5.28634131e-01,  8.48849773e-01,  3.27736855e-01,\n",
       "          9.44769025e-01,  1.98837966e-01,  9.80032384e-01,\n",
       "          1.19712204e-01,  9.92808640e-01,  7.18760788e-02,\n",
       "          9.97413576e-01,  4.31123972e-02,  9.99070227e-01,\n",
       "          2.58503370e-02,  9.99665797e-01,  1.54979751e-02,\n",
       "          9.99879897e-01,  9.29103047e-03,  9.99956846e-01,\n",
       "          5.56987757e-03,  9.99984503e-01,  3.33906501e-03,\n",
       "          9.99994397e-01,  2.00171932e-03,  9.99997973e-01],\n",
       "        [ 4.20167029e-01,  9.07446802e-01,  9.98159170e-01,\n",
       "          6.06491379e-02, -9.99182761e-01, -4.04202044e-02,\n",
       "          3.34267169e-01, -9.42478359e-01,  9.94150102e-01,\n",
       "         -1.08007133e-01,  8.44988048e-01,  5.34785211e-01,\n",
       "          5.67450762e-01,  8.23407352e-01,  3.53895366e-01,\n",
       "          9.35285032e-01,  2.15157464e-01,  9.76579368e-01,\n",
       "          1.29634142e-01,  9.91561890e-01,  7.78540894e-02,\n",
       "          9.96964753e-01,  4.67025824e-02,  9.98908818e-01,\n",
       "          2.80039888e-02,  9.99607801e-01,  1.67893562e-02,\n",
       "          9.99859035e-01,  1.00652575e-02,  9.99949336e-01,\n",
       "          6.03402872e-03,  9.99981821e-01,  3.61731928e-03,\n",
       "          9.99993443e-01,  2.16852897e-03,  9.99997675e-01],\n",
       "        [ 9.90607381e-01,  1.36737213e-01,  8.58326137e-01,\n",
       "         -5.13104558e-01, -9.49565172e-01,  3.13569844e-01,\n",
       "          1.25055820e-01, -9.92149711e-01,  9.71958995e-01,\n",
       "         -2.35150307e-01,  8.83821607e-01,  4.67824042e-01,\n",
       "          6.05045021e-01,  7.96191216e-01,  3.79779875e-01,\n",
       "          9.25076902e-01,  2.31417105e-01,  9.72854614e-01,\n",
       "          1.39543116e-01,  9.90216017e-01,  8.38292986e-02,\n",
       "          9.96480107e-01,  5.02921678e-02,  9.98734534e-01,\n",
       "          3.01575121e-02,  9.99545157e-01,  1.80807095e-02,\n",
       "          9.99836504e-01,  1.08394790e-02,  9.99941230e-01,\n",
       "          6.49817847e-03,  9.99978900e-01,  3.89557332e-03,\n",
       "          9.99992430e-01,  2.33533862e-03,  9.99997258e-01],\n",
       "        [ 6.50287867e-01, -7.59687901e-01,  4.19154823e-01,\n",
       "         -9.07914758e-01, -7.78620780e-01,  6.27494752e-01,\n",
       "         -8.99376869e-02, -9.95947421e-01,  9.33577180e-01,\n",
       "         -3.58376384e-01,  9.17359531e-01,  3.98059726e-01,\n",
       "          6.41336024e-01,  7.67260134e-01,  4.05370325e-01,\n",
       "          9.14152563e-01,  2.47612342e-01,  9.68859196e-01,\n",
       "          1.49438128e-01,  9.88771081e-01,  8.98014978e-02,\n",
       "          9.95959699e-01,  5.38811013e-02,  9.98547375e-01,\n",
       "          3.23108956e-02,  9.99477863e-01,  1.93720330e-02,\n",
       "          9.99812365e-01,  1.16136940e-02,  9.99932587e-01,\n",
       "          6.96232682e-03,  9.99975741e-01,  4.17382689e-03,\n",
       "          9.99991298e-01,  2.50214827e-03,  9.99996841e-01],\n",
       "        [-2.87903309e-01, -9.57659483e-01, -1.66195303e-01,\n",
       "         -9.86092865e-01, -5.08191347e-01,  8.61244202e-01,\n",
       "         -3.00772786e-01, -9.53695834e-01,  8.79643977e-01,\n",
       "         -4.75632668e-01,  9.45400715e-01,  3.25910300e-01,\n",
       "          6.76245570e-01,  7.36676276e-01,  4.30646986e-01,\n",
       "          9.02520478e-01,  2.63738692e-01,  9.64594185e-01,\n",
       "          1.59318209e-01,  9.87227261e-01,  9.57704708e-02,\n",
       "          9.95403469e-01,  5.74693382e-02,  9.98347282e-01,\n",
       "          3.44641283e-02,  9.99405921e-01,  2.06633247e-02,\n",
       "          9.99786496e-01,  1.23879025e-02,  9.99923289e-01,\n",
       "          7.42647378e-03,  9.99972403e-01,  4.45208047e-03,\n",
       "          9.99990106e-01,  2.66895769e-03,  9.99996424e-01],\n",
       "        [-9.61397469e-01, -2.75163352e-01, -6.93585396e-01,\n",
       "         -7.20374465e-01, -1.72829896e-01,  9.84951675e-01,\n",
       "         -4.97701138e-01, -8.67348611e-01,  8.11057806e-01,\n",
       "         -5.84965944e-01,  9.67777193e-01,  2.51808077e-01,\n",
       "          7.09698439e-01,  7.04505563e-01,  4.55590189e-01,\n",
       "          8.90189648e-01,  2.79791653e-01,  9.60060716e-01,\n",
       "          1.69182345e-01,  9.85584795e-01,  1.01736002e-01,\n",
       "          9.94811416e-01,  6.10568337e-02,  9.98134315e-01,\n",
       "          3.66172008e-02,  9.99329388e-01,  2.19545811e-02,\n",
       "          9.99758959e-01,  1.31621026e-02,  9.99913394e-01,\n",
       "          7.89061934e-03,  9.99968886e-01,  4.73033357e-03,\n",
       "          9.99988794e-01,  2.83576711e-03,  9.99996006e-01],\n",
       "        [-7.50987232e-01,  6.60316706e-01, -9.79089916e-01,\n",
       "         -2.03427911e-01,  1.84614182e-01,  9.82811093e-01,\n",
       "         -6.71617508e-01, -7.40898073e-01,  7.28961229e-01,\n",
       "         -6.84554994e-01,  9.84354913e-01,  1.76197037e-01,\n",
       "          7.41622627e-01,  6.70817316e-01,  4.80180681e-01,\n",
       "          8.77169609e-01,  2.95766771e-01,  9.55260158e-01,\n",
       "          1.79029569e-01,  9.83843684e-01,  1.07697874e-01,\n",
       "          9.94183660e-01,  6.46435395e-02,  9.97908413e-01,\n",
       "          3.87701057e-02,  9.99248147e-01,  2.32458003e-02,\n",
       "          9.99729753e-01,  1.39362952e-02,  9.99902904e-01,\n",
       "          8.35476257e-03,  9.99965072e-01,  5.00858575e-03,\n",
       "          9.99987483e-01,  3.00257653e-03,  9.99995470e-01],\n",
       "        [ 1.49877205e-01,  9.88704622e-01, -9.23140228e-01,\n",
       "          3.84463400e-01,  5.18469930e-01,  8.55095863e-01,\n",
       "         -8.14480484e-01, -5.80190897e-01,  6.34721696e-01,\n",
       "         -7.72740841e-01,  9.95034516e-01,  9.95302647e-02,\n",
       "          7.71949291e-01,  6.35684133e-01,  5.04399419e-01,\n",
       "          8.63470435e-01,  3.11659575e-01,  9.50193822e-01,\n",
       "          1.88858896e-01,  9.82004225e-01,  1.13655880e-01,\n",
       "          9.93520200e-01,  6.82294145e-02,  9.97669637e-01,\n",
       "          4.09228280e-02,  9.99162316e-01,  2.45369803e-02,\n",
       "          9.99698937e-01,  1.47104794e-02,  9.99891818e-01,\n",
       "          8.81890487e-03,  9.99961138e-01,  5.28683839e-03,\n",
       "          9.99986053e-01,  3.16938572e-03,  9.99994993e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(20,36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6412ce",
   "metadata": {},
   "source": [
    "### Masking\n",
    ">Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n",
    "\n",
    "*Note: We don't need masking skeleton based Posed based Action Recognition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d4070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b32943",
   "metadata": {},
   "source": [
    "*Similar function in array*\n",
    "<code>\n",
    "    def create_padding_mask(seq):\n",
    "        seq=np.where(seq!=0,0,1)\n",
    "        return seq\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2bd843",
   "metadata": {},
   "source": [
    "*Masking Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "267dfd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd658d",
   "metadata": {},
   "source": [
    "### look ahead mask\n",
    ">The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.<br>This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6065f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea1f6357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "\n",
    "temp = create_look_ahead_mask(x.shape[1]) \n",
    "#Value with 1 are masked \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "02a921ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e4bd8",
   "metadata": {},
   "source": [
    "### Building Transfomrer from scratch\n",
    "i. Multi-Headed Attention Layer <br>\n",
    "1. Linear Layer <br>\n",
    "2. Scaler Dot Product Attention <br>\n",
    "3. Final Linear Layer<br> \n",
    "\n",
    "ii. Encoder <br>\n",
    "iii. Decoder <br>\n",
    "iv. Tranformer Model <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3efe9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    ## shape is determined by simple matrix multiplication rule \n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    #dk is the square root dimension of keys\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f754518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here d_model is feature dimension as well as dimension of the k\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads): ##(self,*,d_model=dimension of k,num_heads)\n",
    "        \n",
    "        ##  d_model refer to the dimension of the q,v,k\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model) ## Must Define dense layer separately for all because Dense() always expect same shape but q,v,k may have different shape\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "#     Split the last dimension into (num_heads, depth).\n",
    "#     Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "#         #because of this split the computational time is same \n",
    "#         #even when the number of heads is increased\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # q.shape= (batch_size, seq_len, d_model) ;Change the last dimesnion to d_model\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model) \n",
    "        ## this must assume the d_model and feature length is same \n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f88c16",
   "metadata": {},
   "source": [
    "*Testing the Above MultiHeadedAttention()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91a3a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([63, 43, 512]), TensorShape([63, 8, 43, 43]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((63,43,64))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebacc06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([63, 43, 64])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcc4a6",
   "metadata": {},
   "source": [
    "### Point wise feed forward network\n",
    ">Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c989348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878d5d3",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "Each encoder layer consists of sublayers:\n",
    "1. Multi-head Attention\n",
    "2. Point wise feed forward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b6ef7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # att_ouptput.shape=(batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        ##d_model must be equal to feature_dimension otherwise error in addition x + attn_output\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "22c6f116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4539e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        ## To embed to d_model\n",
    "        ## test code in below cell\n",
    "        self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)#\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :] #Slices out the shape not in after embedding\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11d6b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try the below code to understand more about embedding and positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea02f4a",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "emb=tf.keras.layers.Embedding(8500, 512)\n",
    "pos=positional_encoding(MAX_TOKENS,512)\n",
    "print('Pos_emb.shape: ',pos.shape)\n",
    "Pos_emb.shape:  (1, 128, 512)\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066220f3",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "x=tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200) #input\n",
    "x=emb(x)\n",
    "print('After Embedding: x.shape ',x.shape)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898ed2d",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "x *= tf.math.sqrt(tf.cast(512, tf.float32))\n",
    "print('x.shape: ',x.shape)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f31b05",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "pos=pos[:,:62,:] # 62 =tf.shape(x)[1] \n",
    "print('Pos_emb.shape: ',pos.shape)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdcdd87",
   "metadata": {},
   "source": [
    "<code>\n",
    "print(x.shape)=TensorShape([64, 62, 512])\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c7f77dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5fe2d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos_emb.shape:  (1, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "emb=tf.keras.layers.Embedding(8500, 512)\n",
    "pos=positional_encoding(MAX_TOKENS,512)\n",
    "print('Pos_emb.shape: ',pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ddb1a",
   "metadata": {},
   "source": [
    "## Decoder layer\n",
    "Each decoder layer consists of sublayers: <br>\n",
    "1. Masked multi-head attention (with look ahead mask and padding mask) \n",
    "2. *** Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.  ***\n",
    "3. Point wise feed forward networks.\n",
    "<br> <br>\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n",
    "\n",
    ">As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output.<b> In other words, the decoder predicts the next token by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ab3d1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) ## v,k,q order  \n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  ## v and k are from encoder \n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b95b0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30779629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                   rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f3e32",
   "metadata": {},
   "source": [
    "### Creating a Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a095b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                   target_vocab_size, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               input_vocab_size=input_vocab_size, rate=rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               target_vocab_size=target_vocab_size, rate=rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return padding_mask, look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af343b",
   "metadata": {},
   "source": [
    "### Setting the HyperParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "651dcb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_attention_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db6ba603",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_attention_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3e333b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSummary of the model: \n",
      "\n",
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_2 (Encoder)         multiple                  1787008   \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  1955584   \n",
      "                                                                 \n",
      " dense_144 (Dense)           multiple                  904290    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,646,882\n",
      "Trainable params: 4,646,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\tSummary of the model: \\n')\n",
    "print(transformer.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a447b",
   "metadata": {},
   "source": [
    "### Test the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b7130c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 7765 and target_vocab_size: 7010\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size=tokenizers.pt.get_vocab_size().numpy()\n",
    "target_vocab_size=tokenizers.en.get_vocab_size().numpy()\n",
    "print('input_vocab_size: {} and target_vocab_size: {}'.format(input_vocab_size,target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c37d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.constant([[1,2,3, 4, 0, 0, 0]])\n",
    "target = tf.constant([[1,2,3, 0]])\n",
    "\n",
    "x, attention = transformer((input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "99522e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 7])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1ca5c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 7010)\n",
      "(1, 8, 4, 4)\n",
      "(1, 8, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(attention['decoder_layer1_block1'].shape)\n",
    "print(attention['decoder_layer4_block2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b9d1e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b714cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This learning rate is according to the paper \"Attention is all we need!\"\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5fedc524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14440408",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Added for testing\n",
    "print(\"Added \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "28351a7ac9c5c3f87d7e2cfd5b1f967c228653fc23bb7a0997ca4e9b6c4c6b30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
